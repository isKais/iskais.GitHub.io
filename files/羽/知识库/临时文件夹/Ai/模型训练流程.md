
模型训练的三个阶段：
模型训练通常包括三个阶段：Self-Supervised Pre-Training（自监督预训练）、Self-Supervised Fine-Tuning（自监督微调）和Reinforcement Learning with Human Feedback（强化学习与人类反馈）。

Self-Supervised Pre-Training (SFT)：
在这个阶段，模型通过大规模的未标记数据(如Masked Language Model（MLM）或Next Sentence Prediction（NSP）)进行自监督预训练，学习捕捉数据中的模式和特征。这有助于模型建立起对数据的基本理解和知识。

Self-Supervised Fine-Tuning (SSFT)：
在自监督微调阶段，模型使用少量标记数据对预训练模型进行微调，以适应特定任务或领域的要求。微调过程有助于提升模型在特定任务上的性能和泛化能力。

Reinforcement Learning with Human Feedback (RLHF)：
在这一阶段，模型通过与人类交互和接收人类反馈来进一步优化和改进。强化学习算法被用于指导模型在与环境互动时学习最优策略，同时结合人类反馈提供额外的指导和调整。

## llama.cpp

https://github.com/ggerganov/llama.cpp

其主要目标是以最少的设置和在广泛的范围内实现 LLM 推理的最先进性能 各种硬件 - 本地和云端。`llama.cpp`
- 没有任何依赖项的普通 C/C++ 实现
- Apple Silicon 是一等公民 - 通过 ARM NEON、Accelerate 和 Metal 框架进行优化
- AVX、AVX2 和 AVX512 支持 x86 架构
- 1.5 位、2 位、3 位、4 位、5 位、6 位和 8 位整数量化，可加快推理速度并减少内存使用
- 用于在 NVIDIA GPU 上运行 LLM 的自定义 CUDA 内核（通过 HIP 支持 AMD GPU）
- Vulkan 和 SYCL 后端支持
- CPU+GPU 混合推理，可部分加速大于总 VRAM 容量的模型
