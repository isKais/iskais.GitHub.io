
如果内存小于模型生成所需, 会非常严重拖慢模型生成速度
一个非常取巧没有实验证明的判断方法是 `模型文件大小*120%` 即为内存需求

如何使得大语言模型能够在低资源的情况下运行?

一般使用量化
quantization
通过降低权重的精度, 牺牲一部分性能, 从而使得模型的推理速度更快, 内存需求更小.

模型一般使用 Float32 储存参数
 Float32(半精度)占用2bytes
 Float32(单精度)占用4bytes
 Float64(双精度)占用8bytes
![[Pasted image 20240821111742.png]]

例如4Bytes储存的2B(亿)模型最小需要 
所以

https://r23456999.medium.com/%E5%A6%82%E4%BD%95%E5%88%A9%E7%94%A8%E9%87%8F%E5%8C%96%E5%9C%A8%E4%BD%8E%E8%B3%87%E6%BA%90%E6%83%85%E6%B3%81%E4%B8%8B%E5%9F%B7%E8%A1%8C70b%E7%9A%84llama2%E6%A8%A1%E5%9E%8B-98691acc7d81

![[1_Ogvtf0QGD4goI2Z_aTHF8g.webp]]

## 不同量化模式
对于不同类型的量化, 可以使用`q+用于存储权重的位数(精度)+特定变体`表示

数字越小, 说明用于数据存储的位数越少, 也就是导致向量空间中的位置准确性降低, 这会影响大模型输出效果

这里有具体的影响表格:
https://www.jamesflare.com/zh-cn/quantization-type-llama-cpp/

这里是网站上抄录的表格
为了帮助我们比较量化类型，我制作了一个表格来显示每种量化类型的大小和 ppl（困惑度）变化。ppl 变化越小意味着质量越好。困惑度通常衡量模型预测结果的置信度。困惑度越低，模型越好。

|Q Type|Size|ppl Change|Note|
|---|---|---|---|
|Q2_K_S|2.16G|+9.0634|@ LLaMA-v1-7B|
|Q2_K|2.63G|+0.6717|@ LLaMA-v1-7B|
|Q3_K_S|2.75G|+0.5551|@ LLaMA-v1-7B|
|Q3_K|-|-|alias for Q3_K_M|
|Q3_K_M|3.07G|+0.2496|@ LLaMA-v1-7B|
|Q3_K_L|3.35G|+0.1764|@ LLaMA-v1-7B|
|Q4_0|3.56G|+0.2166|@ LLaMA-v1-7B|
|Q4_K_S|3.59G|+0.0992|@ LLaMA-v1-7B|
|Q4_K|-|-|alias for Q4_K_M|
|Q4_K_M|3.80G|+0.0532|@ LLaMA-v1-7B|
|Q4_1|3.90G|+0.1585|@ LLaMA-v1-7B|
|Q5_0|4.33G|+0.0683|@ LLaMA-v1-7B|
|Q5_K_S|4.33G|+0.0400|@ LLaMA-v1-7B|
|Q5_1|4.70G|+0.0349|@ LLaMA-v1-7B|
|Q5_K|-|-|alias for Q5_K_M|
|Q5_K_M|4.45G|+0.0122|@ LLaMA-v1-7B|
|Q6_K|5.15G|+0.0008|@ LLaMA-v1-7B|
|Q8_0|6.70G|+0.0004|@ LLaMA-v1-7B|

如果需要自己量化模型, 可以使用llama.cpp

https://huggingface.co/spaces/ggml-org/gguf-my-repo

矩阵量化法 
imatrix优化技术

### 常见的大模型量化方案

#### 1. K-Quants

K-Quants是一种基于KMeans聚类的量化方法。其基本思想是将模型的权重参数映射到K个聚类中心点上，从而用更少的比特数来表示权重。常见的K-Quants有Q8（8-bit）、Q6（6-bit）、Q5（5-bit）等不同的量化级别。
#### 2. imatrix增强的K-Quants

imatrix是一种对K-Quants量化后的模型进行进一步优化的技术。它通过学习量化前后模型输出的差异，建立一个校正矩阵，并将其应用到量化模型中，以补偿量化带来的精度损失。
#### 3. i-quants

i-quants是一种改进的量化方案，通过在量化过程中，联合优化量化中心点和矩阵校正参数，可以达到更好的量化效果。但i-quants目前在业界的应用还比较少见。